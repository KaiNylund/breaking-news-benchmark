{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import hashlib\n",
    "import base64\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from deepmind repo to read gzipped files and output hashes, dates, and base64 text\n",
    "def get_year_hashes(year):\n",
    "  with gzip.open(\"./WMTdata/zipped/news-docs.\" + str(year) + \".en.filtered.gz\", 'rb') as gz_file:\n",
    "    for line in gz_file:\n",
    "      date, sentence_split_text, unsplit_text = line.decode('utf-8').strip().split('\\t')\n",
    "      docid = hashlib.sha256(unsplit_text.encode('utf-8')).hexdigest()\n",
    "      yield docid, (date, sentence_split_text, unsplit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "758958it [01:18, 9674.68it/s] \n",
      "1258408it [02:43, 7685.48it/s] \n",
      "1606654it [03:30, 7636.96it/s] \n",
      "1555033it [03:15, 7955.33it/s] \n"
     ]
    }
   ],
   "source": [
    "# Decode base64 encoded text for the given years and output in csv files with date and docid\n",
    "years = [2012, 2019, 2020, 2021]\n",
    "for year in years:\n",
    "  year_gen = get_year_hashes(year)\n",
    "  # Save a csv with docid, date, and sentence split text cols\n",
    "  with open(\"./WMTdata/decoded_splits_\" + str(year) + \".csv\", \"w\") as splits_csv:\n",
    "    splits_writer = csv.writer(splits_csv, delimiter=\"\\t\")\n",
    "    splits_writer.writerow([\"docid\", \"date\", \"sentence_split_text\"])\n",
    "    for docid, (date, sentence_split_text, unsplit_text) in tqdm(year_gen):\n",
    "      decoded_text = base64.b64decode(sentence_split_text)\n",
    "      splits_writer.writerow([docid, date, decoded_text.decode().replace(\"\\n\", \" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe with all entries from the training data date range\n",
    "splits = []\n",
    "for year in tqdm([2014, 2015, 2016, 2017]):\n",
    "  filepath = \"./WMTdata/decoded_splits_\" + str(year) + \".csv\"\n",
    "  year_splits = pd.read_csv(filepath, delimiter=\"\\t\", header=0, index_col=0)\n",
    "  splits.append(year_splits)\n",
    "splits = pd.concat(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set from provided deepmind hashes\n",
    "with open(\"./WMTdata/train_splits.txt\", \"r\") as train_hashes:\n",
    "  with open(\"./WMTdata/train.txt\", \"w\") as train_file:\n",
    "    for line in train_hashes:\n",
    "      cur_hash = line.split(\" \")[1]\n",
    "      hash_text = splits[cur_hash][\"sentence_split_text\"]\n",
    "      train_file.writerow(hash_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.Random(123)\n",
    "\n",
    "# Create a train data text file from a single year's csv splits\n",
    "def splits_to_text(year, max_articles):\n",
    "  fp = \"./WMTdata/decoded_splits_\" + str(year) + \".csv\"\n",
    "  year_df = pd.read_csv(fp, delimiter=\"\\t\", header=0, index_col=0)\n",
    "  texts = list(year_df.sentence_split_text)\n",
    "  rand.shuffle(texts)\n",
    "  print(\"Total number of texts for year \" + str(year) + \": \" + str(len(texts)))\n",
    "\n",
    "  count = 0\n",
    "  with open(\"./WMTdata/text_\" + str(year) + \".txt\", \"w\") as train_year_file:\n",
    "    for article in tqdm(texts):\n",
    "      train_year_file.write(\"<s>\" + article + \"</s>\")\n",
    "      count += 1\n",
    "      if count > max_articles:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2012, 2022):\n",
    "  splits_to_text(year, 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Since my first run failed after training on 2012\n",
    "fname = \"./metrics/train-year-2012\"\n",
    "#{'train_runtime': 7833.3512, 'train_samples_per_second': 6.778, 'train_steps_per_second': 0.053, 'train_loss': 3.276659389624849, 'epoch': 1.0}\n",
    "metrics = {2012: {'eval_loss': 3.1113810539245605, 'eval_runtime': 266.36, 'eval_samples_per_second': 19.827, 'eval_steps_per_second': 1.243, 'epoch': 1.0},\n",
    "2013: {'eval_loss': 3.2066714763641357, 'eval_runtime': 299.3471, 'eval_samples_per_second': 19.88, 'eval_steps_per_second': 1.243, 'epoch': 1.0},\n",
    "2014: {'eval_loss': 3.1979284286499023, 'eval_runtime': 320.473, 'eval_samples_per_second': 19.874, 'eval_steps_per_second': 1.245, 'epoch': 1.0},\n",
    "2015: {'eval_loss': 3.1861016750335693, 'eval_runtime': 326.2901, 'eval_samples_per_second': 19.875, 'eval_steps_per_second': 1.244, 'epoch': 1.0},\n",
    "2016: {'eval_loss': 3.1606438159942627, 'eval_runtime': 340.249, 'eval_samples_per_second': 19.882, 'eval_steps_per_second': 1.243, 'epoch': 1.0},\n",
    "2017: {'eval_loss': 3.324777364730835, 'eval_runtime': 391.9027, 'eval_samples_per_second': 19.877, 'eval_steps_per_second': 1.243, 'epoch': 1.0},\n",
    "2018: {'eval_loss': 3.3093581199645996, 'eval_runtime': 379.2926, 'eval_samples_per_second': 19.876, 'eval_steps_per_second': 1.244, 'epoch': 1.0},\n",
    "2019: {'eval_loss': 3.247542142868042, 'eval_runtime': 340.0459, 'eval_samples_per_second': 19.883, 'eval_steps_per_second': 1.244, 'epoch': 1.0},\n",
    "2020: {'eval_loss': 3.272118330001831, 'eval_runtime': 341.6526, 'eval_samples_per_second': 19.883, 'eval_steps_per_second': 1.244, 'epoch': 1.0},\n",
    "2021: {'eval_loss': 3.272197723388672, 'eval_runtime': 334.848, 'eval_samples_per_second': 19.881, 'eval_steps_per_second': 1.245, 'epoch': 1.0}}\n",
    "\n",
    "np.save(fname, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "with open(\"./WMTdata/text_2017_100000.txt\", \"r\") as train_year_file:\n",
    "  text = train_year_file.readlines()[0]\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(len(tokens))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "899c603100ad424b173678bf29736c5bd1ec9a65678a0fad157cd1f02f15f28c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
