{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import newspaper\n",
    "from newspaper import news_pool\n",
    "from collections import defaultdict\n",
    "import timeout_decorator\n",
    "import htmldate\n",
    "import lazynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists mainly from https://github.com/yavuz/news-feed-list-of-countries\n",
    "\n",
    "cn_urls = [\"http://news.baidu.com/\", \"http://people.com.cn/\", \"https://www.chinanews.com.cn/\",\n",
    "           \"http://www.cnr.cn/\", \"https://cn.chinadaily.com.cn\"]\n",
    "\n",
    "en_urls = [\"https://www.cnn.com/\", \"https://www.nytimes.com/\", \"https://www.foxnews.com/\",\n",
    "           \"https://www.usatoday.com/\", \"https://abcnews.go.com/\"]\n",
    "\n",
    "# Many articles removed from https://novayagazeta.ru/ (Last major independent paper in russia\n",
    "# to shut down...) after journalists were murdered in response to their coverage of the \n",
    "# war in Ukraine. \n",
    "rs_urls = [\"https://news.mail.ru/\", \"https://novayagazeta.ru/\", \"https://ria.ru/\",\n",
    "           \"https://www.ntv.ru/\", \"http://rbc.ru/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:29<00:00, 29.85s/it]\n",
      "100%|██████████| 5/5 [00:21<00:00,  4.21s/it]]\n",
      "100%|██████████| 5/5 [01:21<00:00, 16.37s/it] \n",
      "100%|██████████| 3/3 [04:12<00:00, 84.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Build papers and keep track of their country of origin\n",
    "lang_urls = [cn_urls, en_urls, rs_urls]\n",
    "lang_abvs = [\"zh\", \"en\", \"ru\"]\n",
    "built_papers = defaultdict(list)\n",
    "\n",
    "tqdm.write(\"Building papers...\")\n",
    "for i in range(len(lang_urls)):\n",
    "  urls = lang_urls[i]\n",
    "  abv = lang_abvs[i]\n",
    "  for url in tqdm(urls):\n",
    "    # memoize_articles = False so we download all recent articles each run\n",
    "    paper = newspaper.build(url, memoize_articles=False)\n",
    "    built_papers[abv].append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Parse articles using purely newspaper3k\n",
    "articles = defaultdict(list)\n",
    "\n",
    "# Set timeout for article.parse in case there are slow GET requests\n",
    "@timeout_decorator.timeout(5, timeout_exception=StopIteration)\n",
    "def saveArticle(article, abv):\n",
    "  article.parse()\n",
    "  articles[abv].append({\"url\": article.url, \"title\": article.title, \n",
    "                     \"authors\": article.authors, \"date\": article.publish_date,\n",
    "                    \"text\": article.text})\n",
    "  np.save(\"articles.npy\", articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading with multiple threads is faster... Here we only use 2 per paper because we don't\n",
    "# want to spam the news sites\n",
    "\n",
    "# Download the papers we just built. NOTE: takes ~15 minutes for RU, ~2 minutes for EN,\n",
    "# and > 2hrs for ZH. Maybe some of the chinese news sites have extra DDos protection\n",
    "# we're running into?\n",
    "tqdm.write(\"Downloading...\")\n",
    "news_pool.set(built_papers, threads_per_source=2) # (15*2) = 30 threads total\n",
    "news_pool.join()\n",
    "tqdm.write(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the articles we just downloaded with newspaper3k. Takes about a day...\n",
    "for abv, papers in built_papers.items():\n",
    "  for paper in papers:\n",
    "    for article in tqdm(paper.articles):\n",
    "      try:\n",
    "        saveArticle(article, abv)\n",
    "      except StopIteration:\n",
    "        tqdm.write(\"Timed out while parsing article\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:18<00:00, 27.78s/it]\n",
      "100%|██████████| 5/5 [00:14<00:00,  2.85s/it]\n",
      "100%|██████████| 5/5 [01:21<00:00, 16.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Build a list of urls + article metadata using newspaper3k.\n",
    "# Then, use lazynlp to download + parse + clean the article text\n",
    "lang_urls = [cn_urls, en_urls, rs_urls]\n",
    "lang_abvs = [\"zh\", \"en\", \"ru\"]\n",
    "built_papers = defaultdict(list)\n",
    "\n",
    "tqdm.write(\"Building papers...\")\n",
    "for i in range(len(lang_urls)):\n",
    "  urls = lang_urls[i]\n",
    "  abv = lang_abvs[i]\n",
    "  for url in tqdm(urls):\n",
    "    # memoize_articles = False so we download all recent articles each run\n",
    "    paper = newspaper.build(url, memoize_articles=False)\n",
    "    built_papers[abv].append(paper)\n",
    "\n",
    "article_urls = defaultdict(list)\n",
    "for abv, papers in built_papers.items():\n",
    "  for paper in papers:\n",
    "    for article in paper.articles:\n",
    "      article_urls[abv].append(article.url)\n",
    "\n",
    "np.save(\"./data/article_urls.npy\", article_urls)\n",
    "print(len(article_urls[\"zh\"]))\n",
    "print(len(article_urls[\"ru\"]))\n",
    "print(len(article_urls[\"en\"]))\n",
    "\n",
    "np.load(\"./data/article_urls.npy\", allow_pickle=True).item()\n",
    "lazy_parsed_articles = defaultdict(list)\n",
    "for abv, abv_urls in article_urls.items():\n",
    "  for url in tqdm(abv_urls):\n",
    "    publish_date = htmldate.find_date(url)\n",
    "    article_text = lazynlp.download_page(url, timeout=5)\n",
    "    lazy_parsed_articles[abv].append({\"url\": url, \"date\": publish_date, \"text\": article_text})\n",
    "\n",
    "np.save(\"lazy_articles.npy\", lazy_parsed_articles)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ea6e71fe6c2aa9f3358b319763c1ad31b448739a273bb69e81d71f2de48b4f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('arkenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
