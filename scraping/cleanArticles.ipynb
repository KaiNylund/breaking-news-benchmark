{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import lazynlp\n",
    "import timeout_decorator\n",
    "import htmldate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'articlesEN = np.load(\"./data/articlesEN.npy\", allow_pickle=True)\\narticlesRU = np.load(\"./data/articlesRU.npy\", allow_pickle=True)\\narticlesZH = np.load(\"./data/articlesZH.npy\", allow_pickle=True)\\n\\nabvs = [\"en\", \"ru\", \"zh\"]\\narticle_dicts = [articlesEN, articlesRU, articlesZH]\\narticles = {}\\n\\nfor i in range(len(abvs)):\\n  abv = abvs[i]\\n  article_dict = article_dicts[i]\\n  filtered_articles = []\\n  for article in article_dict:\\n    if len(article[\"text\"]) > 100:\\n      filtered_dict = {\"url\": article[\"url\"], \"title\": article[\"title\"], \\n                     \"authors\": article[\"authors\"], \"date\": article[\"date\"],\\n                     \"text\": article[\"text\"]}\\n      filtered_articles.append(filtered_dict)\\n  articles[abv] = filtered_articles\\n  print(\"Number of \" + abv + \" articles:\" + str(len(filtered_articles)))\\n  np.save(\"./data/articles.npy\", articles)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temporary block to combine individual files since my parsing failed part of the way through ;-;\n",
    "'''articlesEN = np.load(\"./data/articlesEN.npy\", allow_pickle=True)\n",
    "articlesRU = np.load(\"./data/articlesRU.npy\", allow_pickle=True)\n",
    "articlesZH = np.load(\"./data/articlesZH.npy\", allow_pickle=True)\n",
    "\n",
    "abvs = [\"en\", \"ru\", \"zh\"]\n",
    "article_dicts = [articlesEN, articlesRU, articlesZH]\n",
    "articles = {}\n",
    "\n",
    "for i in range(len(abvs)):\n",
    "  abv = abvs[i]\n",
    "  article_dict = article_dicts[i]\n",
    "  filtered_articles = []\n",
    "  for article in article_dict:\n",
    "    if len(article[\"text\"]) > 100:\n",
    "      filtered_dict = {\"url\": article[\"url\"], \"title\": article[\"title\"], \n",
    "                     \"authors\": article[\"authors\"], \"date\": article[\"date\"],\n",
    "                     \"text\": article[\"text\"]}\n",
    "      filtered_articles.append(filtered_dict)\n",
    "  articles[abv] = filtered_articles\n",
    "  print(\"Number of \" + abv + \" articles:\" + str(len(filtered_articles)))\n",
    "  np.save(\"./data/articles.npy\", articles)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of en articles:1906\n",
      "Number of ru articles:1431\n",
      "Number of zh articles:9627\n"
     ]
    }
   ],
   "source": [
    "# Filter out articles with < 100 characters) and print number of articles for each language\n",
    "articles = np.load(\"./data/articles.npy\", allow_pickle=True).item()\n",
    "for abv, article_list in articles.items():\n",
    "  filtered_articles = []\n",
    "  for article in article_list:\n",
    "    if len(article[\"text\"]) > 100:\n",
    "      filtered_articles.append(article)\n",
    "  articles[abv] = filtered_articles\n",
    "  print(\"Number of \" + abv + \" articles:\" + str(len(filtered_articles)))\n",
    "\n",
    "#np.save(\"./data/filtered_articles.npy\", articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_clean_txt(txt):\n",
    "  txt = lazynlp.clean_html(txt)\n",
    "  txt = lazynlp.transliterate(txt)\n",
    "  txt = lazynlp.collapse_white_spaces(txt)\n",
    "  txt = lazynlp.connect_lines(txt)\n",
    "  txt = lazynlp.replace_unprintable(txt)\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1906/1906 [00:03<00:00, 532.09it/s]\n",
      "100%|██████████| 1431/1431 [00:02<00:00, 495.95it/s]\n",
      "100%|██████████| 9627/9627 [00:11<00:00, 848.08it/s] \n"
     ]
    }
   ],
   "source": [
    "articles = np.load(\"./data/articles.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Extra text clearning step using lazynlp\n",
    "for lang, article_list in articles.items():\n",
    "  for article in tqdm(article_list):\n",
    "    article[\"text\"] = lazy_clean_txt(article[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc=pytz.UTC\n",
    "\n",
    "@timeout_decorator.timeout(5, timeout_exception=StopIteration)\n",
    "def standardize_date(article):\n",
    "  # Try parsing date using htmldate if it's currently none\n",
    "  if article[\"date\"] is None:\n",
    "    try:\n",
    "      article[\"date\"] = datetime.strptime(htmldate.find_date(article[\"url\"]), \"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "      print(\"Error while trying to manually get datetime\")\n",
    "  # If date is not none (after trying htmldate), then convert it to UTC\n",
    "  if article[\"date\"] is not None:\n",
    "    article[\"date\"] = article[\"date\"].replace(tzinfo=utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1906/1906 [00:00<00:00, 9563.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1333/1431 [00:00<00:00, 1547.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n",
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1431/1431 [00:10<00:00, 131.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/9627 [00:10<1:08:52,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 447/9627 [00:20<04:46, 32.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2794/9627 [07:26<33:01,  3.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2854/9627 [07:33<26:11,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 2977/9627 [07:41<14:47,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n",
      "Error while trying to manually get datetime\n",
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 2980/9627 [07:56<54:37,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 6892/9627 [20:25<34:42:20, 45.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to manually get datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9627/9627 [21:10<00:00,  7.58it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Remove all tab and newline characters from text:\n",
    "# Convert all datetimes to UTC:\n",
    "for lang, article_list in articles.items():\n",
    "  for article in tqdm(article_list):\n",
    "    article[\"text\"] = article[\"text\"].replace(\"\\t\", \"\").replace(\"\\n\", \" \")\n",
    "    standardize_date(article)\n",
    "\n",
    "# Convert everything in the articles.npy file to a pandas dataframe\n",
    "article_dfs = {}\n",
    "for lang in articles.keys():\n",
    "  lang_df = pd.DataFrame(articles[lang])\n",
    "  article_dfs[lang] = lang_df\n",
    "  lang_df.to_csv(\"./data/\" + lang + \"_articles.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 50\n",
    "SAMPLE_YEAR = 2021\n",
    "\n",
    "# Randomly sample 50 articles from each month in 2022 (if there are that many)\n",
    "\n",
    "articles_month_dict = {}\n",
    "\n",
    "for lang, article_list in articles.items():\n",
    "  articles_by_month = []\n",
    "  for month in range(12):\n",
    "    articles_by_month.append([])\n",
    "\n",
    "  for article in article_list:\n",
    "    article_date = article[\"date\"]\n",
    "    if (article_date is not None and article_date.year == SAMPLE_YEAR):\n",
    "      articles_by_month[article_date.month - 1].append(article)\n",
    "\n",
    "  articles_month_dict[lang] = articles_by_month\n",
    "\n",
    "# Vast vast majority of articles are from the most recent month... which makes sense\n",
    "# but also means there's probably not enough data to sample accross multiple months into the past\n",
    "for lang in [\"zh\", \"ru\", \"en\"]:\n",
    "  articles_sampled_by_month = np.empty((12, SAMPLE_SIZE), dtype=object)\n",
    "  print(lang + \": \")\n",
    "  for month in range(12):\n",
    "    print(len(articles_month_dict[lang][month]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ea6e71fe6c2aa9f3358b319763c1ad31b448739a273bb69e81d71f2de48b4f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('arkenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
